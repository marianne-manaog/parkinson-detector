{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parkinson's Disease Detector with Apache Cassandra and PySpark Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter notebook inspired by the template at https://github.com/datastaxdevs/workshop-machine-learning/blob/master/jupyter/Random%20Forest.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install matplotlib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install ipykernel --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "Installed kernelspec vs-l-pd-detector in /Users/mariannelynemanaog/Library/Jupyter/kernels/vs-l-pd-detector\n"
     ]
    }
   ],
   "source": [
    "!python3 -m ipykernel install --user --name=vs-l-pd-detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!PYDEVD_DISABLE_FILE_VALIDATION=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cassandra-driver --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import cassandra\n",
    "import pyspark\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "from random import randint, randrange\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to have nicer formatting of Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper for pretty formatting for Spark DataFrames\n",
    "def showDF(df, limitRows =  5, truncate = True):\n",
    "    if(truncate):\n",
    "        pandas.set_option('display.max_colwidth', 50)\n",
    "    else:\n",
    "        pandas.set_option('display.max_colwidth', -1)\n",
    "    pandas.set_option('display.max_rows', limitRows)\n",
    "    display(df.limit(limitRows).toPandas())\n",
    "    pandas.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tables and Loading Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnresolvableContactPoints",
     "evalue": "{}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnresolvableContactPoints\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcassandra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cluster\n\u001b[0;32m----> 3\u001b[0m cluster \u001b[38;5;241m=\u001b[39m \u001b[43mCluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m session \u001b[38;5;241m=\u001b[39m cluster\u001b[38;5;241m.\u001b[39mconnect()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vs-ml-pd-detector/lib/python3.11/site-packages/cassandra/cluster.py:1191\u001b[0m, in \u001b[0;36mcassandra.cluster.Cluster.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnresolvableContactPoints\u001b[0m: {}"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "cluster = Cluster(['dse'])\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Demo Keyspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS accelerate \n",
    "    WITH REPLICATION = \n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set keyspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.set_keyspace('accelerate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table called `speech_data`. Our PRIMARY will be a unique key (subjectId) we generate for each row.  This will have two datasets \"train\" and \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS speech_data \\\n",
    "                                   (subject_id varchar, jitter_percent float, jitter_abs float, rap float, ppq float, \\\n",
    "                                   apq_3 float, apq_5 float, apq_11 float, status int, \\\n",
    "                                   PRIMARY KEY (subject_id))\"\n",
    "session.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the train and test datasets from csv files\n",
    "\n",
    "#### Insert all speech data into the DSE table `speech_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = '/Users/mariannelynemanaog/PycharmProjects/vs-ml-pd-detector/src/data/train_and_test_sets/train_data.csv'\n",
    "input_file = open(fileName, 'r')\n",
    "i = 1\n",
    "for line in input_file:\n",
    "    subject_id = i\n",
    "    row = line.split(';')\n",
    "        \n",
    "    query = \"INSERT INTO speech_data (subject_id, jitter_percent, jitter_abs, rap, ppq, \\\n",
    "                               apq_3, apq_5, apq_11, status)\"\n",
    "    query = query + \" VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "    session.execute(query, (subject_id, float(row[0]), float(row[1]), float(row[2]), float(row[3]), float(row[4]), float(row[5]), float(row[6]), float(row[7])))\n",
    "    i = i + 1\n",
    "\n",
    "fileName = '/Users/mariannelynemanaog/PycharmProjects/vs-ml-pd-detector/src/data/train_and_test_sets/test_data.csv'\n",
    "input_file = open(fileName, 'r')\n",
    "\n",
    "for line in input_file:\n",
    "    subject_id = i\n",
    "    row = line.split(';')\n",
    "        \n",
    "    query = \"INSERT INTO speech_data (subject_id, jitter_percent, jitter_abs, rap, ppq, \\\n",
    "                               apq_3, apq_5, apq_11, status)\"\n",
    "    query = query + \" VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "    session.execute(query, (subject_id, float(row[0]), float(row[1]), float(row[2]), float(row[3]), float(row[4]), float(row[5]), float(row[6]), float(row[7])))\n",
    "    i = i + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with Apache Cassandra and Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a spark session that is connected to the database. From there load each table into a Spark Dataframe and take a count of the number of rows in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('demo').master(\"local\").getOrCreate()\n",
    "\n",
    "\n",
    "speechDF = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"speech_data\", keyspace=\"accelerate\").load()\n",
    "\n",
    "print (\"Table Speech Data Row Count: \")\n",
    "print (speechDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDF(speechDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Vector with all elements of the speech data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['jitter_percent', 'jitter_abs', 'rap', 'ppq', 'apq_3', 'apq_5', 'apq_11'],\n",
    "    outputCol='status')\n",
    "\n",
    "trainingData = assembler.transform(speechDF)\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"status\", outputCol=\"label\", handleInvalid='keep')\n",
    "trainingData1 = labelIndexer.fit(trainingData).transform(trainingData)\n",
    "\n",
    "showDF(trainingData1)\n",
    "print(trainingData1.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be training a model with Random Forest, and because of this we need to split up our dataset in to a training and test set. Will split 80/20. \n",
    "\n",
    "## TODO: To split data based on the train and test sets already determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "splits = trainingData1.randomSplit([0.8, 0.2], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "print (\"Train Dataframe Row Count: \")\n",
    "print (train.count())\n",
    "print (\"Test Datafram Row Count: \")\n",
    "print (test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "model = rf.fit(train)\n",
    "\n",
    "predictions = model.transform(test)\n",
    "#predictions.show()\n",
    "print (predictions.count())\n",
    "showDF(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDF(predictions.select(\"status\", \"label\", \"prediction\", \"probability\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now use the MulticlassClassificationEvaluator to evalute the accuracy of our predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "session.execute(\"\"\"drop table speech_data\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vs-l-pd-detector",
   "language": "python",
   "name": "vs-l-pd-detector"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
